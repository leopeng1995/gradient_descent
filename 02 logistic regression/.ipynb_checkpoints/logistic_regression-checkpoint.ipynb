{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "from mnist import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_features = 12\n",
    "num_rows = 1000\n",
    "seed = 42 # interesting seeds: 28, 32, (42), 56, 58, 63, 91\n",
    "max_epochs = 1000 # for PSO algorithm use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    def __init__(self, position, error, velocity, best_position, best_error):\n",
    "        self.position = position # equivalent to weights\n",
    "        self.error = error # measure of fitness\n",
    "        self.velocity = velocity # determines new position\n",
    "        self.best_position = best_position # best found by this Particle\n",
    "        self.best_error = best_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generation weights:\n",
      "[2.7885359691576745, -9.49978489554666, -4.499413632617615, -5.5357852370235445, 4.729424283280249, 3.533989748458225, 7.843591354096908, -8.261223347411677, -1.561563606294591, -9.404055611238594, -5.627240503927933, 0.10710576206724731, -9.469280606322727]\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "\n",
    "def make_all_data():\n",
    "    weights = [20.0*random.random()-10.0 for i in range(num_features+1)]\n",
    "    # weights = np.array(weights)\n",
    "    result = [[0 for j in range(num_features+1)]\n",
    "              for i in range(num_rows)]\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        y = weights[0]\n",
    "        for j in range(num_features):\n",
    "            x = 20.0*random.random() - 10.0\n",
    "            result[i][j] = x\n",
    "            wx = x * weights[j+1] # weight * x\n",
    "            y += wx\n",
    "            y += num_features*random.random()\n",
    "        if y > num_features:\n",
    "            result[i][num_features] = 1.0\n",
    "        else:\n",
    "            result[i][num_features] = 0.0\n",
    "    \n",
    "    print(\"Data generation weights:\")\n",
    "    print(weights)\n",
    "    \n",
    "    return result\n",
    "\n",
    "all_data = make_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "\n",
      "[9.093842867305796, 8.582512498031448, -2.5154950969167533, -9.227612707342237, 5.56237520665616, 8.203223845987466, -1.2762408148880766, -3.6914724212230894, -6.43479135867405, -6.38399926947293, -3.3642956262958297, 2.7040454220050982, 1.0]\n",
      "[3.6264345924366204, -9.864493165777846, 4.242274277577744, -7.6063323356776085, 2.277009198610296, 4.607606852234484, -5.024009349974701, 5.176420061229196, -6.375238145964213, -0.10049898495270071, -8.217885480201033, 1.480057791013806, 1.0]\n",
      "[-9.924396467712373, 5.772048426322042, -5.731118221069183, 5.469136351103792, 2.508336264596334, 8.272219170373823, -0.6512116690185472, 8.784668903862272, 4.519531355236772, -9.324601746359226, 8.57159574311044, -9.434456985367374, 1.0]\n",
      "...\n",
      "\n",
      "Test data:\n",
      "\n",
      "[9.093842867305796, 8.582512498031448, -2.5154950969167533, -9.227612707342237, 5.56237520665616, 8.203223845987466, -1.2762408148880766, -3.6914724212230894, -6.43479135867405, -6.38399926947293, -3.3642956262958297, 2.7040454220050982, 1.0]\n",
      "[3.6264345924366204, -9.864493165777846, 4.242274277577744, -7.6063323356776085, 2.277009198610296, 4.607606852234484, -5.024009349974701, 5.176420061229196, -6.375238145964213, -0.10049898495270071, -8.217885480201033, 1.480057791013806, 1.0]\n",
      "[-9.924396467712373, 5.772048426322042, -5.731118221069183, 5.469136351103792, 2.508336264596334, 8.272219170373823, -0.6512116690185472, 8.784668903862272, 4.519531355236772, -9.324601746359226, 8.57159574311044, -9.434456985367374, 1.0]\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "\n",
    "def make_train_test():\n",
    "    tot_rows = num_rows\n",
    "    num_train_rows = int(tot_rows * 0.80) # 80% hard-coded\n",
    "    num_test_rows = tot_rows - num_train_rows\n",
    "    \n",
    "    copy_data = copy.copy(all_data)\n",
    "    random.shuffle(copy_data)\n",
    "    \n",
    "    train_data = [copy_data[i] for i in range(num_train_rows)]\n",
    "    test_data = [copy_data[i+num_train_rows] for i in range(num_test_rows)]\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = make_train_test()\n",
    "print(\"Training data:\\n\")\n",
    "for i in range(3):\n",
    "    print(train_data[i])\n",
    "print(\"...\\n\")\n",
    "\n",
    "print(\"Test data:\\n\")\n",
    "for i in range(3):\n",
    "    print(train_data[i])\n",
    "print(\"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LR binary classifier\n",
      "Starting training using no regularization\n",
      "Best weights found:\n",
      "[10.0, -10.0, -10.0, -3.856539591775924, 5.36017129115427, 5.197936689878941, 10.0, -10.0, -1.1583973536907237, -10.0, -9.801043414185703, 0.4598789216573807, -10.0]\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "\n",
    "class LogisticClassifier:\n",
    "    def __init__(self):\n",
    "        self.num_features = num_features\n",
    "        self.weights = [0.0 for i in range(num_features+1)]\n",
    "    \n",
    "    def find_good_L1_weight(self):\n",
    "        result = 0.0\n",
    "        best_err = sys.float_info.max\n",
    "        curr_err = sys.float_info.max\n",
    "        candidates = [0.000, 0.001, 0.005, 0.010, 0.020, 0.050, 0.100, 0.150]\n",
    "\n",
    "        c = LogisticClassifier()\n",
    "        \n",
    "        for trial in range(len(candidates)):\n",
    "            alpha1 = candidates[trial]\n",
    "            wts = c.train(alpha1, 0.0)\n",
    "            curr_err = self.error(wts, 0.0, 0.0)\n",
    "            if curr_err < best_err:\n",
    "                best_err = curr_err\n",
    "                result = candidates[trial]\n",
    "        return result;\n",
    "    \n",
    "    def find_good_L2_weight(self):\n",
    "        result = 0.0\n",
    "        best_err = sys.float_info.max\n",
    "        curr_err = sys.float_info.max\n",
    "        candidates = [0.000, 0.001, 0.005, 0.010, 0.020, 0.050, 0.100, 0.150]\n",
    "\n",
    "        c = LogisticClassifier()\n",
    "        \n",
    "        for trial in range(len(candidates)):\n",
    "            alpha2 = candidates[trial];\n",
    "            wts = c.train(0.0, alpha2)\n",
    "            curr_err = self.error(wts, 0.0, 0.0)\n",
    "            if curr_err < best_err:\n",
    "                best_err = curr_err\n",
    "                result = candidates[trial]\n",
    "\n",
    "        return result;\n",
    "    \n",
    "    def train(self, alpha1, alpha2):\n",
    "        # use PSO. particle position == LR weights\n",
    "        num_particles = 10\n",
    "        prob_death = 0.005\n",
    "        dim = self.num_features + 1 # need one wt for each feature, plus the b0 constant\n",
    "        \n",
    "        epoch = 0\n",
    "        minX = -10.0 # for each weight. assumes data has been normalized about 0\n",
    "        maxX = 10.0\n",
    "        w = 0.729 # inertia weight\n",
    "        c1 = 1.49445 # cognitive/local weight\n",
    "        c2 = 1.49445 # social/global weight\n",
    "        r1, r2 = 0.0, 0.0 # cognitive and social randomizations\n",
    "\n",
    "        swarm = [0.0 for i in range(num_particles)]\n",
    "        # best solution found by any particle in the swarm. implicit initialization to all 0.0\n",
    "        best_swarm_position = [0.0 for i in range(dim)]\n",
    "        best_swarm_error = sys.float_info.max # smaller values better\n",
    "        \n",
    "        for i in range(len(swarm)):\n",
    "            random_position = [0.0 for j in range(dim)]\n",
    "            for j in range(len(random_position)):\n",
    "                random_position[j] = (maxX - minX) * random.random() + minX\n",
    "                \n",
    "            # random_position is a set of weights\n",
    "            error_ = self.error(random_position, alpha1, alpha2)\n",
    "            random_velocity = [0.0 for i in range(dim)]\n",
    "            for j in range(len(random_velocity)):\n",
    "                lo, hi = 0.1 * minX, 0.1 * maxX\n",
    "                random_velocity[j] = (hi - lo) * random.random() + lo\n",
    "            \n",
    "            # last two are best-position and best-error\n",
    "            swarm[i] = Particle(random_position, error_, random_velocity, random_position, error_)\n",
    "\n",
    "            # does current Particle have global best position/solution?\n",
    "            if swarm[i].error < best_swarm_error:\n",
    "                best_swarm_error = swarm[i].error\n",
    "                best_swarm_position = copy.copy(swarm[i].position)\n",
    "            # initialization     \n",
    "            \n",
    "        # main PSO algorithm\n",
    "        sequence = [i for i in range(num_particles)] # process particles in random order\n",
    "            \n",
    "        for epoch in range(max_epochs):\n",
    "            new_velocity = [0.0 for i in range(dim)] # step 1\n",
    "            new_position = [0.0 for i in range(dim)] # step 2\n",
    "            new_error = 0.0 # step 3\n",
    "            random.shuffle(sequence) # move particles in random sequence\n",
    "                \n",
    "            for pi in range(len(swarm)): # each Particle (index)\n",
    "                i = sequence[pi]\n",
    "                currP = swarm[i] # for coding convenience\n",
    "\n",
    "                # 1. compute new velocity\n",
    "                for j in range(len(currP.velocity)): # each x value of the velocity\n",
    "                    r1, r2 = random.random(), random.random()\n",
    "                        \n",
    "                    # velocity depends on old velocity, best position of parrticle, and \n",
    "                    # best position of any particle\n",
    "                    new_velocity[j] = (w * currP.velocity[j]) + (c1 * r1 * (currP.best_position[j] - currP.position[j])) + (c2 * r2 * (best_swarm_position[j] - currP.position[j]))\n",
    "                        \n",
    "                currP.velocity = copy.copy(new_velocity)\n",
    "\n",
    "                # 2. use new velocity to compute new position\n",
    "                for j in range(len(currP.position)):\n",
    "                    new_position[j] = currP.position[j] + new_velocity[j] # compute new position\n",
    "                    if new_position[j] < minX: # keep in range\n",
    "                        new_position[j] = minX\n",
    "                    elif new_position[j] > maxX:\n",
    "                        new_position[j] = maxX\n",
    "\n",
    "                currP.position = copy.copy(new_position)\n",
    "\n",
    "                # 3. use new position to compute new error\n",
    "                new_error = self.error(new_position, alpha1, alpha2)\n",
    "                currP.error = new_error\n",
    "\n",
    "                if new_error < currP.best_error: # new particle best?\n",
    "                    currP.best_position = copy.copy(new_position)\n",
    "                    currP.best_error = new_error\n",
    "\n",
    "                if (new_error < best_swarm_error): # new swarm best?\n",
    "                    best_swarm_position = copy.copy(new_position)\n",
    "                    best_swarm_error = new_error\n",
    "                        \n",
    "                # 4. optional: does curr particle die?\n",
    "                die = random.random()\n",
    "                if die < prob_death:\n",
    "                    # new position, leave velocity, update error\n",
    "                    for j in range(len(currP.position)):\n",
    "                        currP.position[j] = (maxX - minX) * random.random() + minX\n",
    "                    currP.error = self.error(currP.position, alpha1, alpha2)\n",
    "                    currP.best_position = currP.position\n",
    "                    currP.best_error = currP.error\n",
    "\n",
    "                    if currP.error < best_swarm_error: # swarm best by chance?\n",
    "                        best_swarm_error = currP.error\n",
    "                        best_swarm_position = copy.copy(currP.position)\n",
    "        \n",
    "        ret_result = copy.copy(best_swarm_position)\n",
    "        return ret_result\n",
    "    \n",
    "    def accuracy(self, data, weights):\n",
    "        num_correct = 0\n",
    "        num_wrong = 0\n",
    "        y_index = len(data[0]) - 1\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            computed = self.compute_dependent(data[i], weights) # implicit cast\n",
    "            desired = data[i][y_index] # 0.0 or 1.0\n",
    "\n",
    "            epsilon = 0.0000000001\n",
    "            if math.fabs(computed - desired) < epsilon:\n",
    "                num_correct += 1\n",
    "            else:\n",
    "                num_wrong += 1\n",
    "        return (num_correct * 1.0) / (num_wrong + num_correct)\n",
    "    \n",
    "    def error(self, weights, alpha1, alpha2):\n",
    "        # mean squared error using supplied weights\n",
    "        # L1 regularization adds the sum of the absolute values of the weights\n",
    "        # L2 regularization adds the sqrt of sum of squared values\n",
    "\n",
    "        y_index = len(train_data[0]) - 1 # y-value (0/1) is last column\n",
    "        sum_squared_error = 0.0\n",
    "        for i in range(len(train_data)):\n",
    "            computed = self.compute_output(train_data[i], weights)\n",
    "            desired = train_data[i][y_index] # ex: 0.0 or 1.0\n",
    "            sum_squared_error += (computed - desired) * (computed - desired)\n",
    "\n",
    "        sum_abs_vals = 0.0 # L1 penalty\n",
    "        for i in range(len(weights)):\n",
    "            sum_abs_vals += math.fabs(weights[i])\n",
    "\n",
    "        sum_squared_vals = 0.0 # L2 penalty\n",
    "        for i in range(len(weights)):\n",
    "            sum_squared_vals += (weights[i] * weights[i])\n",
    "        # root_sum = math.sqrt(sum_squared_vals)\n",
    "\n",
    "        return (sum_squared_error / len(train_data)) + (alpha1 * sum_abs_vals) + (alpha2 * sum_squared_vals)\n",
    "    \n",
    "    def compute_output(self, data_item, weights):\n",
    "        z = 0.0\n",
    "\n",
    "        z += weights[0] # the b0 constant\n",
    "        for i in range(len(weights)-1): # data might include Y\n",
    "            z += (weights[i + 1] * data_item[i]) # skip first weight\n",
    "        return 1.0 / (1.0 + math.exp(-z))\n",
    "    \n",
    "    def compute_dependent(self, data_item, weights):\n",
    "        sum_ = self.compute_output(data_item, weights)\n",
    "\n",
    "        if sum_ <= 0.5:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "print(\"Creating LR binary classifier\")\n",
    "lc = LogisticClassifier()\n",
    "print(\"Starting training using no regularization\")\n",
    "weights = lc.train(0.0, 0.0)\n",
    "\n",
    "print(\"Best weights found:\")\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy on training data = 0.861250\n",
      "Prediction accuracy on test data = 0.860000\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = lc.accuracy(train_data, weights)\n",
    "print(\"Prediction accuracy on training data = %f\" % train_accuracy)\n",
    "\n",
    "test_accuracy = lc.accuracy(test_data, weights)\n",
    "print(\"Prediction accuracy on test data = %f\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeking good L1 weight\n",
      "Good L1 weight = 0.005000\n"
     ]
    }
   ],
   "source": [
    "print(\"Seeking good L1 weight\")\n",
    "alpha1 = lc.find_good_L1_weight()\n",
    "print(\"Good L1 weight = %f\" % alpha1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using L1 regularization, alpha1 = 0.005000\n",
      "Best weights found:\n",
      "[10.0, -1.3449764559269253, -0.5651064344441007, -0.8314225013136549, 0.6150782132992475, 0.4569079933190636, 1.1203958418682936, -1.0754684033711657, -0.24530382855259114, -1.3605053709213448, -0.8246044782643088, 1.6227453222244882e-14, -1.3093217936868782]\n",
      "Prediction accuracy on training data = 0.973750\n",
      "Prediction accuracy on test data = 0.955000\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training using L1 regularization, alpha1 = %f\" % alpha1)\n",
    "weights = lc.train(alpha1, 0.0)\n",
    "print(\"Best weights found:\")\n",
    "print(weights)\n",
    "\n",
    "train_accuracy = lc.accuracy(train_data, weights)\n",
    "print(\"Prediction accuracy on training data = %f\" % train_accuracy)\n",
    "\n",
    "test_accuracy = lc.accuracy(test_data, weights)\n",
    "print(\"Prediction accuracy on test data = %f\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeking good L2 weight\n",
      "Good L2 weight = 0.001000\n"
     ]
    }
   ],
   "source": [
    "print(\"Seeking good L2 weight\")\n",
    "alpha2 = lc.find_good_L2_weight()\n",
    "print(\"Good L2 weight = %f\" % alpha2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using L2 regularization, alpha1 = 0.001000\n",
      "Best weights found:\n",
      "[3.0117250873983097, -0.4782303023615921, -0.2146089041710627, -0.2695893928788065, 0.24627049656117098, 0.19086912395983371, 0.4147897975541401, -0.4245648558506859, -0.08231130086269411, -0.5110140032755235, -0.3035986751866559, 0.02509893222696457, -0.4708246452708215]\n",
      "Prediction accuracy on training data = 0.971250\n",
      "Prediction accuracy on test data = 0.945000\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training using L2 regularization, alpha1 = %f\" % alpha2)\n",
    "weights = lc.train(0.0, alpha2)\n",
    "print(\"Best weights found:\")\n",
    "print(weights)\n",
    "\n",
    "train_accuracy = lc.accuracy(train_data, weights)\n",
    "print(\"Prediction accuracy on training data = %f\" % train_accuracy)\n",
    "\n",
    "test_accuracy = lc.accuracy(test_data, weights)\n",
    "print(\"Prediction accuracy on test data = %f\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8425330>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [0.0, 0.0, 0.0, 0.1, 0.1, 0.1, 0.1, 0.3, 1] # false_positive_rate\n",
    "y = [0.0, 0.1, 0.2, 0.2, 0.3, 0.4, 0.5, 0.5, 1] # true_positive_rate \n",
    "\n",
    "# This is the ROC curve\n",
    "plt.plot(x,y)\n",
    "plt.show() \n",
    "\n",
    "# This is the AUC\n",
    "auc = np.trapz(y,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### References\n",
    "\n",
    "1. [Particle Swarm Optimization: A Tutorial - Armstrong](http://www.cs.armstrong.edu/saad/csci8100/pso_tutorial.pdf), by James Blondin\n",
    "2. [What is the difference between L1 and L2 regularization? - Quora](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization)\n",
    "3. [Test Run - L1 and L2 Regularization for Machine Learning](https://msdn.microsoft.com/en-us/magazine/dn904675.aspx), by James McCaffrey\n",
    "4. [《统计学习方法》](https://book.douban.com/subject/10590856/)，李航著\n",
    "5. [An Introduction to ROC analysis](https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf), by Tom Fawcett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
