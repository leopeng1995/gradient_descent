{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 12\n",
    "num_rows = 1000\n",
    "seed = 42 # interesting seeds: 28, 32, (42), 56, 58, 63, 91\n",
    "max_epochs = 1000 # for PSO algorithm use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generation weights:\n",
      "[2.7885359691576745, -9.49978489554666, -4.499413632617615, -5.5357852370235445, 4.729424283280249, 3.533989748458225, 7.843591354096908, -8.261223347411677, -1.561563606294591, -9.404055611238594, -5.627240503927933, 0.10710576206724731, -9.469280606322727]\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "\n",
    "def make_all_data():\n",
    "    weights = [20.0*random.random()-10.0 for i in range(num_features+1)]\n",
    "    # weights = np.array(weights)\n",
    "    result = [[0 for j in range(num_features+1)]\n",
    "              for i in range(num_rows)]\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        y = weights[0]\n",
    "        for j in range(num_features):\n",
    "            x = 20.0*random.random() - 10.0\n",
    "            result[i][j] = x\n",
    "            wx = x * weights[j+1] # weight * x\n",
    "            y += wx\n",
    "            y += num_features*random.random()\n",
    "        if y > num_features:\n",
    "            result[i][num_features] = 1.0\n",
    "        else:\n",
    "            result[i][num_features] = 0.0\n",
    "    \n",
    "    print(\"Data generation weights:\")\n",
    "    print(weights)\n",
    "    \n",
    "    return result\n",
    "\n",
    "all_data = make_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "\n",
      "[9.093842867305796, 8.582512498031448, -2.5154950969167533, -9.227612707342237, 5.56237520665616, 8.203223845987466, -1.2762408148880766, -3.6914724212230894, -6.43479135867405, -6.38399926947293, -3.3642956262958297, 2.7040454220050982, 1.0]\n",
      "[3.6264345924366204, -9.864493165777846, 4.242274277577744, -7.6063323356776085, 2.277009198610296, 4.607606852234484, -5.024009349974701, 5.176420061229196, -6.375238145964213, -0.10049898495270071, -8.217885480201033, 1.480057791013806, 1.0]\n",
      "[-9.924396467712373, 5.772048426322042, -5.731118221069183, 5.469136351103792, 2.508336264596334, 8.272219170373823, -0.6512116690185472, 8.784668903862272, 4.519531355236772, -9.324601746359226, 8.57159574311044, -9.434456985367374, 1.0]\n",
      "...\n",
      "\n",
      "Test data:\n",
      "\n",
      "[9.093842867305796, 8.582512498031448, -2.5154950969167533, -9.227612707342237, 5.56237520665616, 8.203223845987466, -1.2762408148880766, -3.6914724212230894, -6.43479135867405, -6.38399926947293, -3.3642956262958297, 2.7040454220050982, 1.0]\n",
      "[3.6264345924366204, -9.864493165777846, 4.242274277577744, -7.6063323356776085, 2.277009198610296, 4.607606852234484, -5.024009349974701, 5.176420061229196, -6.375238145964213, -0.10049898495270071, -8.217885480201033, 1.480057791013806, 1.0]\n",
      "[-9.924396467712373, 5.772048426322042, -5.731118221069183, 5.469136351103792, 2.508336264596334, 8.272219170373823, -0.6512116690185472, 8.784668903862272, 4.519531355236772, -9.324601746359226, 8.57159574311044, -9.434456985367374, 1.0]\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "\n",
    "def make_train_test():\n",
    "    tot_rows = num_rows\n",
    "    num_train_rows = int(tot_rows * 0.80) # 80% hard-coded\n",
    "    num_test_rows = tot_rows - num_train_rows\n",
    "    \n",
    "    copy_data = copy.copy(all_data)\n",
    "    random.shuffle(copy_data)\n",
    "    \n",
    "    train_data = [copy_data[i] for i in range(num_train_rows)]\n",
    "    test_data = [copy_data[i+num_train_rows] for i in range(num_test_rows)]\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = make_train_test()\n",
    "print(\"Training data:\\n\")\n",
    "for i in range(3):\n",
    "    print(train_data[i])\n",
    "print(\"...\\n\")\n",
    "\n",
    "print(\"Test data:\\n\")\n",
    "for i in range(3):\n",
    "    print(train_data[i])\n",
    "print(\"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    def __init__(self, position, error, velocity, best_position, best_error):\n",
    "        self.position = position # equivalent to weights\n",
    "        self.error = error # measure of fitness\n",
    "        self.velocity = velocity # determines new position\n",
    "        self.best_position = best_position # best found by this Particle\n",
    "        self.best_error = best_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LR binary classifier\n",
      "Starting training using no regularization\n",
      "Best weights found:\n",
      "[10.0, -10.0, -10.0, -3.856539591775924, 5.36017129115427, 5.197936689878941, 10.0, -10.0, -1.1583973536907237, -10.0, -9.801043414185703, 0.4598789216573807, -10.0]\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "\n",
    "class LogisticClassifier:\n",
    "    def __init__(self):\n",
    "        self.num_features = num_features\n",
    "        self.weights = [0.0 for i in range(num_features+1)]\n",
    "    \n",
    "    def find_good_L1_weight(self):\n",
    "        result = 0.0\n",
    "        best_err = sys.float_info.max\n",
    "        curr_err = sys.float_info.max\n",
    "        candidates = [0.000, 0.001, 0.005, 0.010, 0.020, 0.050, 0.100, 0.150]\n",
    "\n",
    "        c = LogisticClassifier()\n",
    "        \n",
    "        for trial in range(len(candidates)):\n",
    "            alpha1 = candidates[trial]\n",
    "            wts = c.train(alpha1, 0.0)\n",
    "            curr_err = self.error(wts, 0.0, 0.0)\n",
    "            if curr_err < best_err:\n",
    "                best_err = curr_err\n",
    "                result = candidates[trial]\n",
    "        return result;\n",
    "    \n",
    "    def find_good_L2_weight(self):\n",
    "        result = 0.0\n",
    "        best_err = sys.float_info.max\n",
    "        curr_err = sys.float_info.max\n",
    "        candidates = [0.000, 0.001, 0.005, 0.010, 0.020, 0.050, 0.100, 0.150]\n",
    "\n",
    "        c = LogisticClassifier()\n",
    "        \n",
    "        for trial in range(len(candidates)):\n",
    "            alpha2 = candidates[trial];\n",
    "            wts = c.train(0.0, alpha2)\n",
    "            curr_err = self.error(wts, 0.0, 0.0)\n",
    "            if curr_err < best_err:\n",
    "                best_err = curr_err\n",
    "                result = candidates[trial]\n",
    "\n",
    "        return result;\n",
    "    \n",
    "    def train(self, alpha1, alpha2):\n",
    "        # use PSO. particle position == LR weights\n",
    "        num_particles = 10\n",
    "        prob_death = 0.005\n",
    "        dim = self.num_features + 1 # need one wt for each feature, plus the b0 constant\n",
    "        \n",
    "        epoch = 0\n",
    "        minX = -10.0 # for each weight. assumes data has been normalized about 0\n",
    "        maxX = 10.0\n",
    "        w = 0.729 # inertia weight\n",
    "        c1 = 1.49445 # cognitive/local weight\n",
    "        c2 = 1.49445 # social/global weight\n",
    "        r1, r2 = 0.0, 0.0 # cognitive and social randomizations\n",
    "\n",
    "        swarm = [0.0 for i in range(num_particles)]\n",
    "        # best solution found by any particle in the swarm. implicit initialization to all 0.0\n",
    "        best_swarm_position = [0.0 for i in range(dim)]\n",
    "        best_swarm_error = sys.float_info.max # smaller values better\n",
    "        \n",
    "        for i in range(len(swarm)):\n",
    "            random_position = [0.0 for j in range(dim)]\n",
    "            for j in range(len(random_position)):\n",
    "                random_position[j] = (maxX - minX) * random.random() + minX\n",
    "                \n",
    "            # random_position is a set of weights\n",
    "            error_ = self.error(random_position, alpha1, alpha2)\n",
    "            random_velocity = [0.0 for i in range(dim)]\n",
    "            for j in range(len(random_velocity)):\n",
    "                lo, hi = 0.1 * minX, 0.1 * maxX\n",
    "                random_velocity[j] = (hi - lo) * random.random() + lo\n",
    "            \n",
    "            # last two are best-position and best-error\n",
    "            swarm[i] = Particle(random_position, error_, random_velocity, random_position, error_)\n",
    "\n",
    "            # does current Particle have global best position/solution?\n",
    "            if swarm[i].error < best_swarm_error:\n",
    "                best_swarm_error = swarm[i].error\n",
    "                best_swarm_position = copy.copy(swarm[i].position)\n",
    "            # initialization     \n",
    "            \n",
    "        # main PSO algorithm\n",
    "        sequence = [i for i in range(num_particles)] # process particles in random order\n",
    "            \n",
    "        for epoch in range(max_epochs):\n",
    "            new_velocity = [0.0 for i in range(dim)] # step 1\n",
    "            new_position = [0.0 for i in range(dim)] # step 2\n",
    "            new_error = 0.0 # step 3\n",
    "            random.shuffle(sequence) # move particles in random sequence\n",
    "                \n",
    "            for pi in range(len(swarm)): # each Particle (index)\n",
    "                i = sequence[pi]\n",
    "                currP = swarm[i] # for coding convenience\n",
    "\n",
    "                # 1. compute new velocity\n",
    "                for j in range(len(currP.velocity)): # each x value of the velocity\n",
    "                    r1, r2 = random.random(), random.random()\n",
    "                        \n",
    "                    # velocity depends on old velocity, best position of parrticle, and \n",
    "                    # best position of any particle\n",
    "                    new_velocity[j] = (w * currP.velocity[j]) + (c1 * r1 * (currP.best_position[j] - currP.position[j])) + (c2 * r2 * (best_swarm_position[j] - currP.position[j]))\n",
    "                        \n",
    "                currP.velocity = copy.copy(new_velocity)\n",
    "\n",
    "                # 2. use new velocity to compute new position\n",
    "                for j in range(len(currP.position)):\n",
    "                    new_position[j] = currP.position[j] + new_velocity[j] # compute new position\n",
    "                    if new_position[j] < minX: # keep in range\n",
    "                        new_position[j] = minX\n",
    "                    elif new_position[j] > maxX:\n",
    "                        new_position[j] = maxX\n",
    "\n",
    "                currP.position = copy.copy(new_position)\n",
    "\n",
    "                # 3. use new position to compute new error\n",
    "                new_error = self.error(new_position, alpha1, alpha2)\n",
    "                currP.error = new_error\n",
    "\n",
    "                if new_error < currP.best_error: # new particle best?\n",
    "                    currP.best_position = copy.copy(new_position)\n",
    "                    currP.best_error = new_error\n",
    "\n",
    "                if (new_error < best_swarm_error): # new swarm best?\n",
    "                    best_swarm_position = copy.copy(new_position)\n",
    "                    best_swarm_error = new_error\n",
    "                        \n",
    "                # 4. optional: does curr particle die?\n",
    "                die = random.random()\n",
    "                if die < prob_death:\n",
    "                    # new position, leave velocity, update error\n",
    "                    for j in range(len(currP.position)):\n",
    "                        currP.position[j] = (maxX - minX) * random.random() + minX\n",
    "                    currP.error = self.error(currP.position, alpha1, alpha2)\n",
    "                    currP.best_position = currP.position\n",
    "                    currP.best_error = currP.error\n",
    "\n",
    "                    if currP.error < best_swarm_error: # swarm best by chance?\n",
    "                        best_swarm_error = currP.error\n",
    "                        best_swarm_position = copy.copy(currP.position)\n",
    "        \n",
    "        ret_result = copy.copy(best_swarm_position)\n",
    "        return ret_result\n",
    "    \n",
    "    def accuracy(self, data, weights):\n",
    "        num_correct = 0\n",
    "        num_wrong = 0\n",
    "        y_index = len(data[0]) - 1\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            computed = self.compute_dependent(data[i], weights) # implicit cast\n",
    "            desired = data[i][y_index] # 0.0 or 1.0\n",
    "\n",
    "            epsilon = 0.0000000001\n",
    "            if math.fabs(computed - desired) < epsilon:\n",
    "                num_correct += 1\n",
    "            else:\n",
    "                num_wrong += 1\n",
    "        return (num_correct * 1.0) / (num_wrong + num_correct)\n",
    "    \n",
    "    def error(self, weights, alpha1, alpha2):\n",
    "        # mean squared error using supplied weights\n",
    "        # L1 regularization adds the sum of the absolute values of the weights\n",
    "        # L2 regularization adds the sqrt of sum of squared values\n",
    "\n",
    "        y_index = len(train_data[0]) - 1 # y-value (0/1) is last column\n",
    "        sum_squared_error = 0.0\n",
    "        for i in range(len(train_data)):\n",
    "            computed = self.compute_output(train_data[i], weights)\n",
    "            desired = train_data[i][y_index] # ex: 0.0 or 1.0\n",
    "            sum_squared_error += (computed - desired) * (computed - desired)\n",
    "\n",
    "        sum_abs_vals = 0.0 # L1 penalty\n",
    "        for i in range(len(weights)):\n",
    "            sum_abs_vals += math.fabs(weights[i])\n",
    "\n",
    "        sum_squared_vals = 0.0 # L2 penalty\n",
    "        for i in range(len(weights)):\n",
    "            sum_squared_vals += (weights[i] * weights[i])\n",
    "        # root_sum = math.sqrt(sum_squared_vals)\n",
    "\n",
    "        return (sum_squared_error / len(train_data)) + (alpha1 * sum_abs_vals) + (alpha2 * sum_squared_vals)\n",
    "    \n",
    "    def compute_output(self, data_item, weights):\n",
    "        z = 0.0\n",
    "\n",
    "        z += weights[0] # the b0 constant\n",
    "        for i in range(len(weights)-1): # data might include Y\n",
    "            z += (weights[i + 1] * data_item[i]) # skip first weight\n",
    "        return 1.0 / (1.0 + math.exp(-z))\n",
    "    \n",
    "    def compute_dependent(self, data_item, weights):\n",
    "        sum_ = self.compute_output(data_item, weights)\n",
    "\n",
    "        if sum_ <= 0.5:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "print(\"Creating LR binary classifier\")\n",
    "lc = LogisticClassifier()\n",
    "print(\"Starting training using no regularization\")\n",
    "weights = lc.train(0.0, 0.0)\n",
    "\n",
    "print(\"Best weights found:\")\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy on training data = 0.861250\n",
      "Prediction accuracy on test data = 0.860000\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = lc.accuracy(train_data, weights)\n",
    "print(\"Prediction accuracy on training data = %f\" % train_accuracy)\n",
    "\n",
    "test_accuracy = lc.accuracy(test_data, weights)\n",
    "print(\"Prediction accuracy on test data = %f\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeking good L1 weight\n",
      "Good L1 weight = 0.005000\n"
     ]
    }
   ],
   "source": [
    "print(\"Seeking good L1 weight\")\n",
    "alpha1 = lc.find_good_L1_weight()\n",
    "print(\"Good L1 weight = %f\" % alpha1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using L1 regularization, alpha1 = 0.005000\n",
      "Best weights found:\n",
      "[10.0, -1.3449764559269253, -0.5651064344441007, -0.8314225013136549, 0.6150782132992475, 0.4569079933190636, 1.1203958418682936, -1.0754684033711657, -0.24530382855259114, -1.3605053709213448, -0.8246044782643088, 1.6227453222244882e-14, -1.3093217936868782]\n",
      "Prediction accuracy on training data = 0.973750\n",
      "Prediction accuracy on test data = 0.955000\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training using L1 regularization, alpha1 = %f\" % alpha1)\n",
    "weights = lc.train(alpha1, 0.0)\n",
    "print(\"Best weights found:\")\n",
    "print(weights)\n",
    "\n",
    "train_accuracy = lc.accuracy(train_data, weights)\n",
    "print(\"Prediction accuracy on training data = %f\" % train_accuracy)\n",
    "\n",
    "test_accuracy = lc.accuracy(test_data, weights)\n",
    "print(\"Prediction accuracy on test data = %f\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeking good L2 weight\n",
      "Good L2 weight = 0.001000\n"
     ]
    }
   ],
   "source": [
    "print(\"Seeking good L2 weight\")\n",
    "alpha2 = lc.find_good_L2_weight()\n",
    "print(\"Good L2 weight = %f\" % alpha2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using L2 regularization, alpha1 = 0.001000\n",
      "Best weights found:\n",
      "[3.0117250873983097, -0.4782303023615921, -0.2146089041710627, -0.2695893928788065, 0.24627049656117098, 0.19086912395983371, 0.4147897975541401, -0.4245648558506859, -0.08231130086269411, -0.5110140032755235, -0.3035986751866559, 0.02509893222696457, -0.4708246452708215]\n",
      "Prediction accuracy on training data = 0.971250\n",
      "Prediction accuracy on test data = 0.945000\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training using L2 regularization, alpha1 = %f\" % alpha2)\n",
    "weights = lc.train(0.0, alpha2)\n",
    "print(\"Best weights found:\")\n",
    "print(weights)\n",
    "\n",
    "train_accuracy = lc.accuracy(train_data, weights)\n",
    "print(\"Prediction accuracy on training data = %f\" % train_accuracy)\n",
    "\n",
    "test_accuracy = lc.accuracy(test_data, weights)\n",
    "print(\"Prediction accuracy on test data = %f\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### References\n",
    "\n",
    "1. [Particle Swarm Optimization: A Tutorial - Armstrong](http://www.cs.armstrong.edu/saad/csci8100/pso_tutorial.pdf), by James Blondin\n",
    "2. [What is the difference between L1 and L2 regularization? - Quora](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization)\n",
    "3. [Test Run - L1 and L2 Regularization for Machine Learning](https://msdn.microsoft.com/en-us/magazine/dn904675.aspx), by James McCaffrey\n",
    "4. [《统计学习方法》，李航著](https://book.douban.com/subject/10590856/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
